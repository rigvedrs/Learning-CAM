{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%pip install grad-cam","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### The thing with object detection models is that GradCams require gradients, and since these models give out outputs not in image format but the locations and scores for the classes, they become non differentiable. So this means there won't be any generic way of doing that. So we are gonna need some other method for this.\n#### So CAM methods can be divided into two groups:\n- #### Those that need gradients like GradCAM, GradCAM++, LayerCAM, etc.\n- #### Those that don't need gradients like AblationCAM, ScoreCAM, and EigenCAM.","metadata":{}},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')\nwarnings.simplefilter('ignore')\nimport cv2\nimport numpy as np\nimport torch\nimport torchvision\nfrom pytorch_grad_cam import AblationCAM, EigenCAM\nfrom pytorch_grad_cam.ablation_layer import AblationLayerFasterRCNN\nfrom pytorch_grad_cam.utils.model_targets import FasterRCNNBoxScoreTarget\nfrom pytorch_grad_cam.utils.reshape_transforms import fasterrcnn_reshape_transform\nfrom pytorch_grad_cam.utils.image import show_cam_on_image, scale_accross_batch_and_channels, scale_cam_image","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def predict(input_tensor, model, device, detection_threshold):\n    outputs = model(input_tensor)\n    pred_classes = [coco_names[i] for i in outputs[0]['labels'].cpu().numpy()]\n    pred_labels = outputs[0]['labels'].cpu().numpy()\n    pred_scores = outputs[0]['scores'].detach().cpu().numpy()\n    pred_bboxes = outputs[0]['boxes'].detach().cpu().numpy()\n    \n    boxes, classes, labels, indices = [], [], [], []\n    for index in range(len(pred_scores)):\n        if pred_scores[index] >= detection_threshold:\n            boxes.append(pred_bboxes[index].astype(np.int32))\n            classes.append(pred_classes[index])\n            labels.append(pred_labels[index])\n            indices.append(index)\n    boxes = np.int32(boxes)\n    return boxes, classes, labels, indices\n\ndef draw_boxes(boxes, labels, classes, image):\n    for i, box in enumerate(boxes):\n        color = COLORS[labels[i]]\n        cv2.rectangle(\n            image,\n            (int(box[0]), int(box[1])),\n            (int(box[2]), int(box[3])),\n            color, 2\n        )\n        cv2.putText(image, classes[i], (int(box[0]), int(box[1] - 5)),\n                    cv2.FONT_HERSHEY_SIMPLEX, 0.8, color, 2,\n                    lineType=cv2.LINE_AA)\n    return image\n\ncoco_names = ['__background__', 'person', 'bicycle', 'car', 'motorcycle', 'airplane', \\\n              'bus', 'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'N/A', \n              'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', \n              'cow', 'elephant', 'bear', 'zebra', 'giraffe', 'N/A', 'backpack', 'umbrella',\n              'N/A', 'N/A', 'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard',\n              'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard',\n              'surfboard', 'tennis racket', 'bottle', 'N/A', 'wine glass', 'cup', 'fork',\n              'knife', 'spoon', 'bowl', 'banana', 'apple', 'sandwich', 'orange',\n              'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair', 'couch',\n              'potted plant', 'bed', 'N/A', 'dining table', 'N/A', 'N/A', 'toilet',\n              'N/A', 'tv', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone', 'microwave',\n              'oven', 'toaster', 'sink', 'refrigerator', 'N/A', 'book', 'clock', 'vase',\n              'scissors', 'teddy bear', 'hair drier', 'toothbrush']\n\n\n# This will help us create a different color for each class\nCOLORS = np.random.uniform(0, 255, size=(len(coco_names), 3))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import requests\nimport torchvision\nfrom PIL import Image\nimage_url = \"https://raw.githubusercontent.com/jacobgil/pytorch-grad-cam/master/examples/both.png\"\nimage = np.array(Image.open(requests.get(image_url, stream=True).raw))\nimage_float_np = np.float32(image) / 255\n# define the torchvision image transforms\ntransform = torchvision.transforms.Compose([\n    torchvision.transforms.ToTensor(),\n])\n\ninput_tensor = transform(image)\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\ninput_tensor = input_tensor.to(device)\n# Add a batch dimension:\ninput_tensor = input_tensor.unsqueeze(0)\n\nmodel = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\nmodel.eval().to(device)\n\n# Run the model and display the detections\nboxes, classes, labels, indices = predict(input_tensor, model, device, 0.9)\nimage = draw_boxes(boxes, labels, classes, image)\n\n# Show the image:\nImage.fromarray(image)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Now that we have a working model with the necessary functions, we can now decide which layers to target from this model. Since the final layers of FPN mainly deal with giving the box indices for the categories, we don't have much use of them. \n#### We need the layers that can show us what the model is looking at in the image. The faster-RCNN has FPN backbone as it's model.backbone, which does the more meaningful computations on the image. Everything else after that basically uses the output from this backbone to extract the features and then predict the bounding boxes.","metadata":{}},{"cell_type":"code","source":"b_output = model.backbone(input_tensor)\nprint(type(output))\nprint(b_output.keys())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### The backbone outputs on Ordered dictionary with 5 different tensors. Each tensor has different shape. We want to aggregrate these image tensors, assign them weights, and then aggregrate everything. To do this we will write a custom function that takes these tensors, resizes them to a common shape and concatenates them. ","metadata":{}},{"cell_type":"code","source":"def fasterrcnn_reshape_transform(x):\n    target_size = x['pool'].size()[-2 : ]\n    activations = []\n    for key, value in x.items():\n        activations.append(torch.nn.functional.interpolate(torch.abs(value), target_size, mode='bilinear'))\n    activations = torch.cat(activations, axis=1)\n    return activations","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### We also used the abs() function on the activations since the FPN activations are unbounded and can have negative values as well.","metadata":{}},{"cell_type":"code","source":"fasterrcnn_reshape_transform(b_output).shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Now the class activation maps are created for the specific targets. How will we do this for this model. It was much easier for the classifier model since we could easily separate them out. \n#### Here, the model outputs bounding boxes and their categories of those only for the highest scoring categories, so we are going to have to limit ourselves to that.\n\n#### The next problem to solve is being able to detect how the detected objects correspond with the original detections in both coordinates and score\n\n#### Now to assign the output some score, we will first find each detection that overlaps with the bounding boxes from the original detections the most. After that we will then apply some conditional checks on it.\n- **Check if the IOU of the detection with the original detection is high enough - if not, assign a score of 0 to the box**.\n\n- **Check if the category is still the same as in the original detection - if not, assign a score of 0 to the box**.\n\n- **Find the classification score**.\n\n- **box_score = IOU + classification_score**.\n\n**target = sum(box_scores)**","metadata":{}},{"cell_type":"code","source":"class FasterRCNNBoxScoreTarget:\n    \"\"\" For every original detected bounding box specified in \"bounding boxes\",\n        assign a score on how the current bounding boxes match it,\n            1. In IOU\n            2. In the classification score.\n            If there is not a large enough overlap, or the category changed,\n            assign a score of 0.\n\n    The total score is the sum of all the box scores.\n    \"\"\"\n\n    def __init__(self, labels, bounding_boxes, iou_threshold=0.5):\n        self.labels = labels\n        self.bounding_boxes = bounding_boxes\n        self.iou_threshold = iou_threshold\n\n    def __call__(self, model_outputs):\n        output = torch.Tensor([0])\n        if torch.cuda.is_available():\n            output = output.cuda()\n\n        if len(model_outputs[\"boxes\"]) == 0:\n            return output\n\n        for box, label in zip(self.bounding_boxes, self.labels):\n            box = torch.Tensor(box[None, :])\n            if torch.cuda.is_available():\n                box = box.cuda()\n\n            ious = torchvision.ops.box_iou(box, model_outputs[\"boxes\"])\n            index = ious.argmax()\n            if ious[0, index] > self.iou_threshold and model_outputs[\"labels\"][index] == label:\n                score = ious[0, index] + model_outputs[\"scores\"][index]\n                output = output + score\n        return output","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"target_layers = [model.backbone]\ntargets = [FasterRCNNBoxScoreTarget(labels=labels, bounding_boxes=boxes)]\ncam = EigenCAM(model,\n               target_layers, \n               use_cuda=torch.cuda.is_available(),\n               reshape_transform=fasterrcnn_reshape_transform)\n\ngrayscale_cam = cam(input_tensor, targets=targets)\n# Take the first image in the batch:\ngrayscale_cam = grayscale_cam[0, :]\ncam_image = show_cam_on_image(image_float_np, grayscale_cam, use_rgb=True)\n# And lets draw the boxes again:\nimage_with_bounding_boxes = draw_boxes(boxes, labels, classes, cam_image)\nImage.fromarray(image_with_bounding_boxes)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"np.shape(np.repeat(cam(input_tensor, targets=targets), 3, axis=0))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nplt.imshow(np.repeat(cam(input_tensor, targets=targets), 3, axis=0))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}